---
title: "Homework 5"
author: Amanda Warnock
output: github_document
---

This is my solution to HW5.

```{r}
library(tidyverse)
library(rvest)
library(ggplot2)
library(data.table)
library(patchwork)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_color_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```


## Problem 1

Read in the data.
```{r}
homicide_df = 
  read_csv("./data/homicide-data.csv") %>% 
  mutate(
   city_state = str_c(city, state, sep = "_"),
   resolved = case_when(
     disposition == "Closed without arrest" ~ "unsolved",
     disposition == "Open/No arrest"        ~ "unsolved",
     disposition == "Closed by arrest"      ~ "solved"
   ) 
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

Let's look at this a bit.

```{r}
aggregate_df = 
homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Can I do a prop test for a single city?

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

Try to iterate .....

```{r}
results_df = 
aggregate_df %>% 
  mutate(
    prop_test = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_test = map(.x = prop_test, ~broom::tidy(.x))
  ) %>% 
  select(-prop_test) %>% 
  unnest(tidy_test) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) 
```

## Problem 2

Import the 20 files, clean, and tidy.

```{r}
path_df = 
  tibble(
    path = list.files("./lddata")
  ) %>% 
  mutate(
    path = str_c("lddata/", path),
    path_names = path,
    data = map(path, read_csv)
    ) %>% 
  separate(col = path_names, into = c("path_1", "path_2"), sep = 7, remove = T) %>% 
  separate(col = path_2, into = c("arm", "ID"), sep = 3) %>% 
  separate(col = ID, into = c("underscore", "ID"), sep = 1, remove = T) %>% 
  separate(col = ID, into = c("ID", "csv"), sep = 2, remove = T) %>% 
  select(-path_1, -underscore, -csv) %>% 
  mutate(
    arm = str_replace(arm, "con", "Control"),
    arm = str_replace(arm, "exp", "Experiment")) %>% 
  unnest(data) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week", 
    values_to = "observation"
  ) 
```

Make a spaghetti plot showing observations 
```{r}
path_plot = 
  path_df %>% 
  unite("Arm_ID", arm:ID, sep = "_", remove = FALSE) %>% 
  ggplot(aes(x = week, y = observation, group = Arm_ID, color = arm)) +
  geom_line() +
  labs(title = "Observations per Subject Over Time")

path_plot
```

This plot shows that overall, the observations for the subjects in the experiment group were higher than the observations for the control group. Though the starting observations for the subjects were mixed at week 1, all observations for the experiment group were higher than all observations for the control group at week 8. 

## Problem 3

```{r}
sim_ttest = function(n, mu = 0, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = sigma),
  )
  
    ttest=t.test(sim_data, mu=0, sd=5)
    ttest[['p.value']]
    ttest[['estimate']]
    
  sim_results = tibble(
     pvalue = ttest[['p.value']],
     mean = ttest[['estimate']]
  )

}

output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = sim_ttest(30) 
}
sim_results = bind_rows(output)
```

Testing the simulation in rerun. 

```{r}
sim_results1 = 
  rerun(5000, sim_ttest(30, 1, 5)) %>% 
  bind_rows()
```

Simulating across multiple means. 

```{r}
m_list = 
  list(
    "m_0" = 0,
    "m_1" = 1,
    "m_2" = 2,
    "m_3" = 3,
    "m_4" = 4,
    "m_5" = 5,
    "m_6" = 6
  )

output = vector("list", length = 7)

for (i in 1:7) {
  output[[i]] = rerun(5000, sim_ttest(30, m_list[[i]], 5)) %>% 
    bind_rows()
}
sim_full_results=bind_rows(output) 
```

```{r}
sim_full_results = 
  tibble(mus = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = mus, ~rerun(5000, sim_ttest(30, .x, 5))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```

Plots
```{r}
results_df = 
  sim_full_results %>% 
  mutate(
    decision = case_when(
      pvalue >= 0.05 ~ "fail to reject",
      pvalue < 0.05 ~ "reject"
    )
  ) %>% 
  filter(decision == "reject") %>% 
  group_by(mus) %>% 
  summarize(power = n()/5000)

plot_1 =
results_df %>% 
  ggplot(aes(x = mus, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Proportion of times the null is rejected",
    x = "True Value of Mu",
    y = "Power")

plot_1
 ```

As the Mu increases from 0 to 6, the proportion of the time that the null hypothesis is rejected increases. Effect size and power increase together as we increase Mus. 

 
```{r}
plot_all = 
sim_full_results %>% 
  mutate(
    mus = str_c("Mu = ", mus),
    mus = fct_inorder(mus)) %>% 
  ggplot(aes(x = mus, y = mean, fill = mus)) +
  geom_violin() +
  labs(title = "Estimate and Mu in all stamples") +
  stat_summary(fun=mean, geom="point",  size=1)

plot_reject =
sim_full_results %>% 
  mutate(
    mus = str_c("Mu = ", mus),
    mus = fct_inorder(mus)) %>% 
  filter(pvalue < 0.05) %>% 
  ggplot(aes(x = mus, y = mean, fill = mus)) +
  geom_violin() +
  labs(title = "Estimate and Mu only where null is rejected") +
  stat_summary(fun=mean, geom="point", size=1)

plot_all + plot_reject
```

In cases where the null is rejected, the average mu-hat is about equal to the true value of Mu because the power of the test is so high given the huge sample size and the large effect size. However, it is more accurate when the whole sample is included. 
